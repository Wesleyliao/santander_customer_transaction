{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train.csv')\n",
    "test_df = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 202) (200000, 201)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = train_df.drop(['target','ID_code'], axis = 1)\n",
    "train_labels = train_df['target']\n",
    "test_features = test_df.drop(['ID_code'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "        'bagging_freq': 5,\n",
    "        'bagging_fraction': 0.38,\n",
    "        'boost_from_average':'false',\n",
    "        'boost': 'gbdt',\n",
    "        'feature_fraction': 0.045,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': -1,  \n",
    "        'metric':'auc',\n",
    "        'min_data_in_leaf': 80,\n",
    "        'min_sum_hessian_in_leaf': 10.0,\n",
    "        'num_leaves': 13,\n",
    "        'num_threads': 4,\n",
    "        'tree_learner': 'serial',\n",
    "        'objective': 'binary', \n",
    "        'verbosity': 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_round = 100000\n",
    "kfold = 15\n",
    "folds = StratifiedKFold(n_splits=kfold, shuffle=False, random_state=333)\n",
    "oof = np.zeros(len(train_df))\n",
    "predictions_lgb = np.zeros(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900513\tvalid_1's auc: 0.88338\n",
      "[2000]\ttraining's auc: 0.911164\tvalid_1's auc: 0.89165\n",
      "[3000]\ttraining's auc: 0.918219\tvalid_1's auc: 0.896051\n",
      "[4000]\ttraining's auc: 0.923726\tvalid_1's auc: 0.898287\n",
      "[5000]\ttraining's auc: 0.928367\tvalid_1's auc: 0.899791\n",
      "[6000]\ttraining's auc: 0.93243\tvalid_1's auc: 0.900464\n",
      "[7000]\ttraining's auc: 0.936127\tvalid_1's auc: 0.901371\n",
      "[8000]\ttraining's auc: 0.939649\tvalid_1's auc: 0.901657\n",
      "[9000]\ttraining's auc: 0.943064\tvalid_1's auc: 0.902047\n",
      "[10000]\ttraining's auc: 0.946264\tvalid_1's auc: 0.902382\n",
      "[11000]\ttraining's auc: 0.949375\tvalid_1's auc: 0.902402\n",
      "[12000]\ttraining's auc: 0.952391\tvalid_1's auc: 0.902436\n",
      "[13000]\ttraining's auc: 0.955217\tvalid_1's auc: 0.902494\n",
      "[14000]\ttraining's auc: 0.957985\tvalid_1's auc: 0.902719\n",
      "[15000]\ttraining's auc: 0.960678\tvalid_1's auc: 0.902886\n",
      "[16000]\ttraining's auc: 0.963208\tvalid_1's auc: 0.902854\n",
      "[17000]\ttraining's auc: 0.965608\tvalid_1's auc: 0.902803\n",
      "[18000]\ttraining's auc: 0.967928\tvalid_1's auc: 0.90266\n",
      "Early stopping, best iteration is:\n",
      "[15344]\ttraining's auc: 0.961559\tvalid_1's auc: 0.902945\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900912\tvalid_1's auc: 0.879801\n",
      "[2000]\ttraining's auc: 0.911764\tvalid_1's auc: 0.885576\n",
      "[3000]\ttraining's auc: 0.918826\tvalid_1's auc: 0.888967\n",
      "[4000]\ttraining's auc: 0.92425\tvalid_1's auc: 0.89101\n",
      "[5000]\ttraining's auc: 0.928893\tvalid_1's auc: 0.892448\n",
      "[6000]\ttraining's auc: 0.932914\tvalid_1's auc: 0.893559\n",
      "[7000]\ttraining's auc: 0.93659\tvalid_1's auc: 0.894369\n",
      "[8000]\ttraining's auc: 0.940083\tvalid_1's auc: 0.894442\n",
      "[9000]\ttraining's auc: 0.943466\tvalid_1's auc: 0.894559\n",
      "[10000]\ttraining's auc: 0.946679\tvalid_1's auc: 0.894472\n",
      "[11000]\ttraining's auc: 0.949762\tvalid_1's auc: 0.894561\n",
      "[12000]\ttraining's auc: 0.952774\tvalid_1's auc: 0.894714\n",
      "[13000]\ttraining's auc: 0.955639\tvalid_1's auc: 0.894512\n",
      "[14000]\ttraining's auc: 0.958406\tvalid_1's auc: 0.894413\n",
      "[15000]\ttraining's auc: 0.961091\tvalid_1's auc: 0.89428\n",
      "Early stopping, best iteration is:\n",
      "[12157]\ttraining's auc: 0.95323\tvalid_1's auc: 0.894825\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900198\tvalid_1's auc: 0.88729\n",
      "[2000]\ttraining's auc: 0.911007\tvalid_1's auc: 0.89409\n",
      "[3000]\ttraining's auc: 0.918071\tvalid_1's auc: 0.897976\n",
      "[4000]\ttraining's auc: 0.923509\tvalid_1's auc: 0.900284\n",
      "[5000]\ttraining's auc: 0.928135\tvalid_1's auc: 0.901786\n",
      "[6000]\ttraining's auc: 0.932216\tvalid_1's auc: 0.902658\n",
      "[7000]\ttraining's auc: 0.93596\tvalid_1's auc: 0.903261\n",
      "[8000]\ttraining's auc: 0.939481\tvalid_1's auc: 0.903546\n",
      "[9000]\ttraining's auc: 0.942913\tvalid_1's auc: 0.90377\n",
      "[10000]\ttraining's auc: 0.946127\tvalid_1's auc: 0.903598\n",
      "[11000]\ttraining's auc: 0.949243\tvalid_1's auc: 0.903608\n",
      "[12000]\ttraining's auc: 0.952301\tvalid_1's auc: 0.903689\n",
      "Early stopping, best iteration is:\n",
      "[9218]\ttraining's auc: 0.94364\tvalid_1's auc: 0.903847\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901427\tvalid_1's auc: 0.872108\n",
      "[2000]\ttraining's auc: 0.912161\tvalid_1's auc: 0.878973\n",
      "[3000]\ttraining's auc: 0.919143\tvalid_1's auc: 0.882956\n",
      "[4000]\ttraining's auc: 0.924574\tvalid_1's auc: 0.885108\n",
      "[5000]\ttraining's auc: 0.929229\tvalid_1's auc: 0.886712\n",
      "[6000]\ttraining's auc: 0.933273\tvalid_1's auc: 0.887182\n",
      "[7000]\ttraining's auc: 0.93698\tvalid_1's auc: 0.887796\n",
      "[8000]\ttraining's auc: 0.940478\tvalid_1's auc: 0.887964\n",
      "[9000]\ttraining's auc: 0.943874\tvalid_1's auc: 0.888342\n",
      "[10000]\ttraining's auc: 0.947034\tvalid_1's auc: 0.888271\n",
      "[11000]\ttraining's auc: 0.950063\tvalid_1's auc: 0.888106\n",
      "[12000]\ttraining's auc: 0.953049\tvalid_1's auc: 0.888099\n",
      "Early stopping, best iteration is:\n",
      "[9634]\ttraining's auc: 0.945896\tvalid_1's auc: 0.888457\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.899854\tvalid_1's auc: 0.890494\n",
      "[2000]\ttraining's auc: 0.91082\tvalid_1's auc: 0.899623\n",
      "[3000]\ttraining's auc: 0.917883\tvalid_1's auc: 0.904417\n",
      "[4000]\ttraining's auc: 0.923441\tvalid_1's auc: 0.906798\n",
      "[5000]\ttraining's auc: 0.928003\tvalid_1's auc: 0.907777\n",
      "[6000]\ttraining's auc: 0.932019\tvalid_1's auc: 0.908317\n",
      "[7000]\ttraining's auc: 0.935792\tvalid_1's auc: 0.90844\n",
      "[8000]\ttraining's auc: 0.939349\tvalid_1's auc: 0.90845\n",
      "[9000]\ttraining's auc: 0.942789\tvalid_1's auc: 0.908562\n",
      "[10000]\ttraining's auc: 0.946023\tvalid_1's auc: 0.908422\n",
      "[11000]\ttraining's auc: 0.949173\tvalid_1's auc: 0.908435\n",
      "[12000]\ttraining's auc: 0.952196\tvalid_1's auc: 0.908145\n",
      "Early stopping, best iteration is:\n",
      "[9037]\ttraining's auc: 0.942911\tvalid_1's auc: 0.908581\n",
      "Fold 5\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900591\tvalid_1's auc: 0.876206\n",
      "[2000]\ttraining's auc: 0.911753\tvalid_1's auc: 0.88511\n",
      "[3000]\ttraining's auc: 0.918881\tvalid_1's auc: 0.888572\n",
      "[4000]\ttraining's auc: 0.924394\tvalid_1's auc: 0.890507\n",
      "[5000]\ttraining's auc: 0.928912\tvalid_1's auc: 0.891625\n",
      "[6000]\ttraining's auc: 0.932971\tvalid_1's auc: 0.892156\n",
      "[7000]\ttraining's auc: 0.936642\tvalid_1's auc: 0.89211\n",
      "[8000]\ttraining's auc: 0.940137\tvalid_1's auc: 0.892496\n",
      "[9000]\ttraining's auc: 0.943514\tvalid_1's auc: 0.892536\n",
      "[10000]\ttraining's auc: 0.94666\tvalid_1's auc: 0.892652\n",
      "[11000]\ttraining's auc: 0.949764\tvalid_1's auc: 0.892721\n",
      "[12000]\ttraining's auc: 0.952732\tvalid_1's auc: 0.892589\n",
      "[13000]\ttraining's auc: 0.955592\tvalid_1's auc: 0.892607\n",
      "[14000]\ttraining's auc: 0.958317\tvalid_1's auc: 0.892634\n",
      "Early stopping, best iteration is:\n",
      "[11172]\ttraining's auc: 0.950282\tvalid_1's auc: 0.892798\n",
      "Fold 6\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900179\tvalid_1's auc: 0.882417\n",
      "[2000]\ttraining's auc: 0.91115\tvalid_1's auc: 0.891225\n",
      "[3000]\ttraining's auc: 0.918311\tvalid_1's auc: 0.89523\n",
      "[4000]\ttraining's auc: 0.923889\tvalid_1's auc: 0.897677\n",
      "[5000]\ttraining's auc: 0.928487\tvalid_1's auc: 0.89916\n",
      "[6000]\ttraining's auc: 0.93254\tvalid_1's auc: 0.899829\n",
      "[7000]\ttraining's auc: 0.936343\tvalid_1's auc: 0.899966\n",
      "[8000]\ttraining's auc: 0.939907\tvalid_1's auc: 0.900299\n",
      "[9000]\ttraining's auc: 0.943294\tvalid_1's auc: 0.900487\n",
      "[10000]\ttraining's auc: 0.946528\tvalid_1's auc: 0.900568\n",
      "[11000]\ttraining's auc: 0.949634\tvalid_1's auc: 0.900499\n",
      "[12000]\ttraining's auc: 0.952676\tvalid_1's auc: 0.900614\n",
      "[13000]\ttraining's auc: 0.955531\tvalid_1's auc: 0.900501\n",
      "Early stopping, best iteration is:\n",
      "[10715]\ttraining's auc: 0.948773\tvalid_1's auc: 0.900652\n",
      "Fold 7\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900475\tvalid_1's auc: 0.884875\n",
      "[2000]\ttraining's auc: 0.911233\tvalid_1's auc: 0.8926\n",
      "[3000]\ttraining's auc: 0.918392\tvalid_1's auc: 0.896444\n",
      "[4000]\ttraining's auc: 0.923958\tvalid_1's auc: 0.898297\n",
      "[5000]\ttraining's auc: 0.928574\tvalid_1's auc: 0.89975\n",
      "[6000]\ttraining's auc: 0.932693\tvalid_1's auc: 0.900236\n",
      "[7000]\ttraining's auc: 0.936411\tvalid_1's auc: 0.900788\n",
      "[8000]\ttraining's auc: 0.939856\tvalid_1's auc: 0.901074\n",
      "[9000]\ttraining's auc: 0.943261\tvalid_1's auc: 0.900986\n",
      "[10000]\ttraining's auc: 0.946414\tvalid_1's auc: 0.901023\n",
      "[11000]\ttraining's auc: 0.949547\tvalid_1's auc: 0.900656\n",
      "[12000]\ttraining's auc: 0.952521\tvalid_1's auc: 0.900783\n",
      "Early stopping, best iteration is:\n",
      "[9580]\ttraining's auc: 0.945095\tvalid_1's auc: 0.901107\n",
      "Fold 8\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.899797\tvalid_1's auc: 0.889495\n",
      "[2000]\ttraining's auc: 0.910851\tvalid_1's auc: 0.897747\n",
      "[3000]\ttraining's auc: 0.918008\tvalid_1's auc: 0.90085\n",
      "[4000]\ttraining's auc: 0.923558\tvalid_1's auc: 0.903203\n",
      "[5000]\ttraining's auc: 0.928142\tvalid_1's auc: 0.904114\n",
      "[6000]\ttraining's auc: 0.932264\tvalid_1's auc: 0.904504\n",
      "[7000]\ttraining's auc: 0.936009\tvalid_1's auc: 0.904892\n",
      "[8000]\ttraining's auc: 0.939466\tvalid_1's auc: 0.905143\n",
      "[9000]\ttraining's auc: 0.942857\tvalid_1's auc: 0.905055\n",
      "[10000]\ttraining's auc: 0.946053\tvalid_1's auc: 0.904734\n",
      "[11000]\ttraining's auc: 0.949166\tvalid_1's auc: 0.904319\n",
      "Early stopping, best iteration is:\n",
      "[8414]\ttraining's auc: 0.940925\tvalid_1's auc: 0.905186\n",
      "Fold 9\n",
      "Training until validation scores don't improve for 3000 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's auc: 0.900034\tvalid_1's auc: 0.888621\n",
      "[2000]\ttraining's auc: 0.910996\tvalid_1's auc: 0.894956\n",
      "[3000]\ttraining's auc: 0.918157\tvalid_1's auc: 0.89849\n",
      "[4000]\ttraining's auc: 0.923677\tvalid_1's auc: 0.900483\n",
      "[5000]\ttraining's auc: 0.928371\tvalid_1's auc: 0.901463\n",
      "[6000]\ttraining's auc: 0.932469\tvalid_1's auc: 0.901887\n",
      "[7000]\ttraining's auc: 0.936197\tvalid_1's auc: 0.902004\n",
      "[8000]\ttraining's auc: 0.939625\tvalid_1's auc: 0.901983\n",
      "[9000]\ttraining's auc: 0.942996\tvalid_1's auc: 0.902062\n",
      "[10000]\ttraining's auc: 0.946206\tvalid_1's auc: 0.902094\n",
      "[11000]\ttraining's auc: 0.949366\tvalid_1's auc: 0.901836\n",
      "Early stopping, best iteration is:\n",
      "[8410]\ttraining's auc: 0.94105\tvalid_1's auc: 0.902148\n",
      "Fold 10\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900563\tvalid_1's auc: 0.878374\n",
      "[2000]\ttraining's auc: 0.911319\tvalid_1's auc: 0.887064\n",
      "[3000]\ttraining's auc: 0.918456\tvalid_1's auc: 0.891282\n",
      "[4000]\ttraining's auc: 0.923959\tvalid_1's auc: 0.893882\n",
      "[5000]\ttraining's auc: 0.928528\tvalid_1's auc: 0.895292\n",
      "[6000]\ttraining's auc: 0.93252\tvalid_1's auc: 0.896217\n",
      "[7000]\ttraining's auc: 0.936286\tvalid_1's auc: 0.89661\n",
      "[8000]\ttraining's auc: 0.939789\tvalid_1's auc: 0.89685\n",
      "[9000]\ttraining's auc: 0.943144\tvalid_1's auc: 0.89702\n",
      "[10000]\ttraining's auc: 0.946371\tvalid_1's auc: 0.897014\n",
      "[11000]\ttraining's auc: 0.949501\tvalid_1's auc: 0.897055\n",
      "[12000]\ttraining's auc: 0.952448\tvalid_1's auc: 0.897216\n",
      "[13000]\ttraining's auc: 0.955329\tvalid_1's auc: 0.89721\n",
      "[14000]\ttraining's auc: 0.958085\tvalid_1's auc: 0.897046\n",
      "[15000]\ttraining's auc: 0.960678\tvalid_1's auc: 0.896891\n",
      "Early stopping, best iteration is:\n",
      "[12615]\ttraining's auc: 0.954237\tvalid_1's auc: 0.897359\n",
      "Fold 11\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900204\tvalid_1's auc: 0.885504\n",
      "[2000]\ttraining's auc: 0.910891\tvalid_1's auc: 0.893486\n",
      "[3000]\ttraining's auc: 0.918092\tvalid_1's auc: 0.897809\n",
      "[4000]\ttraining's auc: 0.923671\tvalid_1's auc: 0.900292\n",
      "[5000]\ttraining's auc: 0.928238\tvalid_1's auc: 0.901791\n",
      "[6000]\ttraining's auc: 0.932321\tvalid_1's auc: 0.902697\n",
      "[7000]\ttraining's auc: 0.936079\tvalid_1's auc: 0.903035\n",
      "[8000]\ttraining's auc: 0.939603\tvalid_1's auc: 0.903192\n",
      "[9000]\ttraining's auc: 0.942984\tvalid_1's auc: 0.903188\n",
      "[10000]\ttraining's auc: 0.946196\tvalid_1's auc: 0.903116\n",
      "[11000]\ttraining's auc: 0.949329\tvalid_1's auc: 0.903029\n",
      "Early stopping, best iteration is:\n",
      "[8433]\ttraining's auc: 0.941093\tvalid_1's auc: 0.903391\n",
      "Fold 12\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900173\tvalid_1's auc: 0.888392\n",
      "[2000]\ttraining's auc: 0.911\tvalid_1's auc: 0.895268\n",
      "[3000]\ttraining's auc: 0.918167\tvalid_1's auc: 0.899493\n",
      "[4000]\ttraining's auc: 0.923711\tvalid_1's auc: 0.901349\n",
      "[5000]\ttraining's auc: 0.9283\tvalid_1's auc: 0.902579\n",
      "[6000]\ttraining's auc: 0.932373\tvalid_1's auc: 0.902929\n",
      "[7000]\ttraining's auc: 0.936127\tvalid_1's auc: 0.903336\n",
      "[8000]\ttraining's auc: 0.939627\tvalid_1's auc: 0.903419\n",
      "[9000]\ttraining's auc: 0.943012\tvalid_1's auc: 0.903421\n",
      "[10000]\ttraining's auc: 0.946224\tvalid_1's auc: 0.903469\n",
      "[11000]\ttraining's auc: 0.949288\tvalid_1's auc: 0.903279\n",
      "[12000]\ttraining's auc: 0.952274\tvalid_1's auc: 0.903172\n",
      "Early stopping, best iteration is:\n",
      "[9263]\ttraining's auc: 0.943888\tvalid_1's auc: 0.903628\n",
      "Fold 13\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.899761\tvalid_1's auc: 0.894028\n",
      "[2000]\ttraining's auc: 0.910628\tvalid_1's auc: 0.90171\n",
      "[3000]\ttraining's auc: 0.917782\tvalid_1's auc: 0.905992\n",
      "[4000]\ttraining's auc: 0.923295\tvalid_1's auc: 0.908148\n",
      "[5000]\ttraining's auc: 0.9279\tvalid_1's auc: 0.909514\n",
      "[6000]\ttraining's auc: 0.931945\tvalid_1's auc: 0.909939\n",
      "[7000]\ttraining's auc: 0.935718\tvalid_1's auc: 0.910304\n",
      "[8000]\ttraining's auc: 0.939184\tvalid_1's auc: 0.910532\n",
      "[9000]\ttraining's auc: 0.942508\tvalid_1's auc: 0.910619\n",
      "[10000]\ttraining's auc: 0.945762\tvalid_1's auc: 0.910775\n",
      "[11000]\ttraining's auc: 0.948951\tvalid_1's auc: 0.910672\n",
      "[12000]\ttraining's auc: 0.952012\tvalid_1's auc: 0.910523\n",
      "[13000]\ttraining's auc: 0.954852\tvalid_1's auc: 0.910425\n",
      "Early stopping, best iteration is:\n",
      "[10052]\ttraining's auc: 0.945929\tvalid_1's auc: 0.910826\n",
      "Fold 14\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.90063\tvalid_1's auc: 0.88243\n",
      "[2000]\ttraining's auc: 0.911553\tvalid_1's auc: 0.888501\n",
      "[3000]\ttraining's auc: 0.918726\tvalid_1's auc: 0.892168\n",
      "[4000]\ttraining's auc: 0.924234\tvalid_1's auc: 0.893995\n",
      "[5000]\ttraining's auc: 0.928843\tvalid_1's auc: 0.894893\n",
      "[6000]\ttraining's auc: 0.93289\tvalid_1's auc: 0.895414\n",
      "[7000]\ttraining's auc: 0.936673\tvalid_1's auc: 0.895902\n",
      "[8000]\ttraining's auc: 0.940141\tvalid_1's auc: 0.895957\n",
      "[9000]\ttraining's auc: 0.943504\tvalid_1's auc: 0.895999\n",
      "[10000]\ttraining's auc: 0.94668\tvalid_1's auc: 0.895929\n",
      "[11000]\ttraining's auc: 0.94976\tvalid_1's auc: 0.896031\n",
      "Early stopping, best iteration is:\n",
      "[8924]\ttraining's auc: 0.943262\tvalid_1's auc: 0.89607\n",
      "CV score: 0.90056 \n"
     ]
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, train_labels.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train_features.iloc[trn_idx], label=train_labels.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train_features.iloc[val_idx], label=train_labels.iloc[val_idx])\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n",
    "    oof[val_idx] = clf.predict(train_features.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "    predictions_lgb += clf.predict(test_features, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    \n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(train_labels, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n",
    "submission[\"target\"] = predictions_lgb\n",
    "submission.to_csv(\"submission_lgb2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Simple_NN(nn.Module):\n",
    "    def __init__(self ,input_dim ,hidden_dim, dropout = 0.1):\n",
    "        super(Simple_NN, self).__init__()\n",
    "        \n",
    "        self.inpt_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, int(hidden_dim/2))\n",
    "        self.fc4 = nn.Linear(int(hidden_dim/2), int(hidden_dim/2))\n",
    "        self.fc5 = nn.Linear(int(hidden_dim/2), int(hidden_dim/4))\n",
    "        self.fc6 = nn.Linear(int(hidden_dim/4), int(hidden_dim/8))\n",
    "        self.fc7 = nn.Linear(int(hidden_dim/8), 1)\n",
    "#         self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "#         self.bn2 = nn.BatchNorm1d((hidden_dim))\n",
    "#         self.bn3 = nn.BatchNorm1d(int(hidden_dim/4))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        y = self.relu(y)\n",
    "        #y = self.bn1(y)\n",
    "        #y = self.dropout(y)\n",
    "        \n",
    "        y = self.fc2(y)\n",
    "        y = self.relu(y)\n",
    "        #y = self.bn2(y)\n",
    "        #y = self.dropout(y)\n",
    "        \n",
    "        y = self.fc3(y)\n",
    "        y = self.relu(y)\n",
    "        #y = self.bn3(y)\n",
    "        #y = self.dropout(y)\n",
    "        \n",
    "        y = self.fc4(y)\n",
    "        y = self.relu(y)\n",
    "        #y = self.bn3(y)\n",
    "        #y = self.dropout(y)\n",
    "        \n",
    "        y = self.fc5(y)\n",
    "        y = self.relu(y)\n",
    "        #y = self.bn3(y)\n",
    "        #y = self.dropout(y)\n",
    "        \n",
    "        y = self.fc6(y)\n",
    "        y = self.relu(y)\n",
    "        #y = self.bn3(y)\n",
    "        #y = self.dropout(y)\n",
    "        \n",
    "        out= self.fc7(y)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Simple_NN(200,1024)\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/40 \t loss=0.2834 \t val_loss=0.2940 \t time=7.72s\n",
      "Epoch 2/40 \t loss=0.2562 \t val_loss=0.3178 \t time=7.62s\n",
      "Epoch 3/40 \t loss=0.2532 \t val_loss=0.2841 \t time=7.60s\n",
      "Epoch 4/40 \t loss=0.2581 \t val_loss=0.2962 \t time=7.63s\n",
      "Epoch 5/40 \t loss=0.2547 \t val_loss=0.2817 \t time=7.66s\n",
      "Epoch 6/40 \t loss=0.2563 \t val_loss=0.2772 \t time=7.66s\n",
      "Epoch 7/40 \t loss=0.2490 \t val_loss=0.2932 \t time=7.63s\n",
      "Epoch 8/40 \t loss=0.2453 \t val_loss=0.2852 \t time=7.69s\n",
      "Epoch 9/40 \t loss=0.2410 \t val_loss=0.2711 \t time=7.58s\n",
      "Epoch 10/40 \t loss=0.2376 \t val_loss=0.2771 \t time=7.65s\n",
      "Epoch 11/40 \t loss=0.2348 \t val_loss=0.2693 \t time=7.64s\n",
      "Epoch 12/40 \t loss=0.2320 \t val_loss=0.2688 \t time=8.14s\n",
      "Epoch 13/40 \t loss=0.2343 \t val_loss=0.2708 \t time=7.76s\n",
      "Epoch 14/40 \t loss=0.2356 \t val_loss=0.2758 \t time=7.73s\n",
      "Epoch 15/40 \t loss=0.2362 \t val_loss=0.2777 \t time=8.91s\n",
      "Epoch 16/40 \t loss=0.2385 \t val_loss=0.2698 \t time=8.53s\n",
      "Epoch 17/40 \t loss=0.2370 \t val_loss=0.2790 \t time=8.47s\n",
      "Epoch 18/40 \t loss=0.2369 \t val_loss=0.2675 \t time=8.45s\n",
      "Epoch 19/40 \t loss=0.2343 \t val_loss=0.2747 \t time=8.51s\n",
      "Epoch 20/40 \t loss=0.2319 \t val_loss=0.2688 \t time=9.14s\n",
      "Epoch 21/40 \t loss=0.2279 \t val_loss=0.2688 \t time=8.78s\n",
      "Epoch 22/40 \t loss=0.2250 \t val_loss=0.2691 \t time=9.74s\n",
      "Epoch 23/40 \t loss=0.2219 \t val_loss=0.2718 \t time=7.75s\n",
      "Epoch 24/40 \t loss=0.2222 \t val_loss=0.2732 \t time=7.78s\n",
      "Epoch 25/40 \t loss=0.2243 \t val_loss=0.2827 \t time=7.90s\n",
      "Epoch 26/40 \t loss=0.2266 \t val_loss=0.2752 \t time=8.79s\n",
      "Epoch 27/40 \t loss=0.2279 \t val_loss=0.2760 \t time=7.71s\n",
      "Epoch 28/40 \t loss=0.2300 \t val_loss=0.2836 \t time=8.57s\n",
      "Epoch 29/40 \t loss=0.2288 \t val_loss=0.2849 \t time=8.28s\n",
      "Epoch 30/40 \t loss=0.2270 \t val_loss=0.2860 \t time=7.76s\n",
      "Epoch 31/40 \t loss=0.2243 \t val_loss=0.2777 \t time=7.65s\n",
      "Epoch 32/40 \t loss=0.2216 \t val_loss=0.2846 \t time=7.67s\n",
      "Epoch 33/40 \t loss=0.2167 \t val_loss=0.2822 \t time=7.64s\n",
      "Epoch 34/40 \t loss=0.2129 \t val_loss=0.2849 \t time=8.63s\n",
      "Epoch 35/40 \t loss=0.2112 \t val_loss=0.2860 \t time=7.68s\n",
      "Epoch 36/40 \t loss=0.2140 \t val_loss=0.2894 \t time=7.67s\n",
      "Epoch 37/40 \t loss=0.2158 \t val_loss=0.2808 \t time=7.61s\n",
      "Epoch 38/40 \t loss=0.2166 \t val_loss=0.2800 \t time=7.63s\n",
      "Epoch 39/40 \t loss=0.2190 \t val_loss=0.3016 \t time=7.75s\n",
      "Epoch 40/40 \t loss=0.2196 \t val_loss=0.2805 \t time=7.74s\n",
      "Fold 2\n",
      "Epoch 1/40 \t loss=0.2160 \t val_loss=0.2388 \t time=7.77s\n",
      "Epoch 2/40 \t loss=0.2179 \t val_loss=0.2568 \t time=7.81s\n",
      "Epoch 3/40 \t loss=0.2222 \t val_loss=0.2510 \t time=7.79s\n",
      "Epoch 4/40 \t loss=0.2256 \t val_loss=0.2532 \t time=7.71s\n",
      "Epoch 5/40 \t loss=0.2270 \t val_loss=0.2574 \t time=7.74s\n",
      "Epoch 6/40 \t loss=0.2293 \t val_loss=0.2599 \t time=7.63s\n",
      "Epoch 7/40 \t loss=0.2278 \t val_loss=0.2601 \t time=7.80s\n",
      "Epoch 8/40 \t loss=0.2232 \t val_loss=0.2570 \t time=7.76s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-2a746eb7403a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m#avg_auc = 0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 512\n",
    "\n",
    "train_preds = np.zeros((len(train_features)))\n",
    "test_preds = np.zeros((len(test_features)))\n",
    "\n",
    "x_test = np.array(test_features)\n",
    "x_test_cuda = torch.tensor(x_test, dtype=torch.float).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "avg_losses_f = []\n",
    "avg_val_losses_f = []\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(list(folds.split(train_features, train_labels))):  \n",
    "    x_train = np.array(train_features)\n",
    "    y_train = np.array(train_labels)\n",
    "    \n",
    "    x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.float).cuda()\n",
    "    y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.float).cuda()\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    step_size = 2000\n",
    "    base_lr, max_lr = 0.0001, 0.005  \n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=max_lr)\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    ################################################################################################\n",
    "    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "               step_size=step_size, mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "    ###############################################################################################\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        #avg_auc = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            #########################\n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "            ########################\n",
    "            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()/len(train_loader)\n",
    "            #avg_auc += round(roc_auc_score(y_batch.cpu(),y_pred.detach().cpu()),4) / len(train_loader)\n",
    "        model.eval()\n",
    "        \n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros((len(test_features)))\n",
    "        \n",
    "        avg_val_loss = 0.\n",
    "        #avg_val_auc = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            \n",
    "            #avg_val_auc += round(roc_auc_score(y_batch.cpu(),sigmoid(y_pred.cpu().numpy())[:, 0]),4) / len(valid_loader)\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "        \n",
    "    avg_losses_f.append(avg_loss)\n",
    "    avg_val_losses_f.append(avg_val_loss) \n",
    "    \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / folds.n_splits\n",
    "\n",
    "auc_nn  =  round(roc_auc_score(train_labels,train_preds),4)      \n",
    "print('All \\t loss={:.4f} \\t val_loss={:.4f} \\t auc={:.4f}'.format(np.average(avg_losses_f),np.average(avg_val_losses_f),auc_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n",
    "submission[\"target\"] = test_preds\n",
    "submission.to_csv(\"submission_NN.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_train_pred = 0.5*oof + 0.5*train_preds\n",
    "ensemble_test_pred = 0.5*predictions_lgb + 0.5*test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"LightGBM AUC: {:<8.5f}\".format(roc_auc_score(train_labels, oof)))\n",
    "print(\"NN AUC: {:<8.5f}\".format(roc_auc_score(train_labels,train_preds)))\n",
    "print(\"LightGBM AUC: {:<8.5f}\".format(roc_auc_score(train_labels, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n",
    "submission[\"target\"] = ensemble_test_pred\n",
    "submission.to_csv(\"submission_ensemble.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
